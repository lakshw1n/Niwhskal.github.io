<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-04-05T14:51:23+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lakshwin Shreesha</title><subtitle>Lakshwin Shreesha's blog</subtitle><author><name>Lakshwin Shreesha</name></author><entry><title type="html">Towards Extreme Generalization</title><link href="http://localhost:4000/Towards-Extreme_Generalization/" rel="alternate" type="text/html" title="Towards Extreme Generalization" /><published>2021-04-04T00:00:00+05:30</published><updated>2021-04-04T00:00:00+05:30</updated><id>http://localhost:4000/Towards-Extreme_Generalization</id><content type="html" xml:base="http://localhost:4000/Towards-Extreme_Generalization/">&lt;p&gt;In 2019, François Chollet provided one of the most comprehensive definitions of intelligence. He described intelligence as that which enables wild generalization across tasks, optimized specifically for flexibility and adaptability. &lt;a class=&quot;citation&quot; href=&quot;#fchol&quot;&gt;(Chollet, 2019)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately trends in ML research today are a far cry from his description, leveraging unlimited priors to showcase superior performance at specific tasks, depicting the intelligence of the person involved in creating it rather than that of the algorithm. All of the SOTA in ML research today showcases the crystallized output of intelligence but not the process of intelligence itself.&lt;/p&gt;

&lt;p&gt;This brings us to the question of what the way forward is? Are neural networks the key to superior generalization or do we need a new framework altogether?&lt;/p&gt;

&lt;p&gt;In my opinion, neural networks, atleast in the framework of: Training, Back-Prop, and Optimization are quite useless to achieve the state of superior generalization.&lt;/p&gt;

&lt;p&gt;An agent which serves to generalize across multiple tasks needs to have a sense of uncertainty about it’s priors. It must have the ability to decrease/increase that uncertainty on it’s own, by a process which is independent of data. In order to escape from the bubble that unlimited training data provides, an agent needs to cut off it’s reliance with data. Biological organisms seeem to do fine in this regard, so why can’t ML agents?&lt;/p&gt;

&lt;p&gt;The brain is a master of dealing with uncertainty. If we draw a superficial comparison with neural networks and the brain, you can see that they’re both connected roughly the same. The complexity of their connections obscure both their dynamics (not for the lack of trying though &lt;a class=&quot;citation&quot; href=&quot;#dyn1&quot;&gt;(Cessac B, 2007)&lt;/a&gt;,&lt;a class=&quot;citation&quot; href=&quot;#dyn2&quot;&gt;(Tian, 2017)&lt;/a&gt;,&lt;a class=&quot;citation&quot; href=&quot;#dyn3&quot;&gt;(Jacot et al., 2020)&lt;/a&gt;,&lt;a class=&quot;citation&quot; href=&quot;#dyn4&quot;&gt;(Tachet et al., 2020)&lt;/a&gt;,&lt;a class=&quot;citation&quot; href=&quot;#dyn5&quot;&gt;(Kunin et al., 2021)&lt;/a&gt; ), so we don’t know how an input gets transformed as it passes through several layers. There are signifcant differences though. The brain doesn’t work on back propagation &lt;a class=&quot;citation&quot; href=&quot;#bp1&quot;&gt;(Roelfsema and Holtmaat, 2018)&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#bp2&quot;&gt;(Whittington and Bogacz, 2019)&lt;/a&gt; and has a control over it’s experiences. It can assign relevance/determine validity because everything is uncertain, it is too fluid of an entity. Neural networks share the property of intractable dynamics with the brain but, they are forced to behave in a pre-determined way by the means of labels. Hard-coded labels enforce unknown constraints on the network, forcing it to either learn shortcut tricks (textures, patterns, colors etc.). If a network is trained solely to detect pictures of faces, it’s behavior is morphed so as to “see” faces, whether it be in patterns, shortcut hacks or some other feature. So when it makes a mistake, it technically isn’t a mistake from the perspective of the neural network, but a mismatch in perspective between what is true and what isn’t. And this has to do with assigning labels. We need to rethink the way a neural network leans. A more flexible way. One which adapts constantly based on new data or ingrained priors(designed before training). Pre-defined datasets and hard-coded labels are the death of generalization.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;hr /&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;fchol&quot;&gt;Chollet, F., 2019. On the Measure of Intelligence.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dyn1&quot;&gt;Cessac B, S.M., 2007. From neuron to neural network dynamics. Eur. Phys. J. Spec. Top 142.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dyn2&quot;&gt;Tian, Y., 2017. An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dyn3&quot;&gt;Jacot, A., Gabriel, F., Hongler, C., 2020. Neural Tangent Kernel: Convergence and Generalization in Neural Networks.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dyn4&quot;&gt;Tachet, R., Pezeshki, M., Shabanian, S., Courville, A., Bengio, Y., 2020. On the Learning Dynamics of Deep Neural Networks.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dyn5&quot;&gt;Kunin, D., Sagastuy-Brena, J., Ganguli, S., Yamins, D.L.K., Tanaka, H., 2021. Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bp1&quot;&gt;Roelfsema, P., Holtmaat, A., 2018. Control of synaptic plasticity in deep cortical networks. Nature Reviews Neuroscience 19, 166–180. https://doi.org/10.1038/nrn.2018.6&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bp2&quot;&gt;Whittington, J., Bogacz, R., 2019. Theories of Error Back-Propagation in the Brain. Trends in Cognitive Sciences 23. https://doi.org/10.1016/j.tics.2018.12.005&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Lakshwin Shreesha</name></author><summary type="html">In 2019, François Chollet provided one of the most comprehensive definitions of intelligence. He described intelligence as that which enables wild generalization across tasks, optimized specifically for flexibility and adaptability. (Chollet, 2019)</summary></entry></feed>